<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D Gaussians">
  <meta name="keywords" content="3D Generation, Novel View Synthesis, 3D Gaussian
  Splatting">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Can3Tok</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <script type="text/x-mathjax-config"> 
        MathJax.Hub.Config({ TeX: { equationNumbers: { autoNumber: "all" } } }); 
  </script>
  <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {
              inlineMath: [ ['$','$'], ["\\(","\\)"] ],
              processEscapes: true
            }
        });
  </script>

  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript">
  </script>


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/usc_logo.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D Gaussians</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zerg-overmind.github.io/">Quankai Gao</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="https://iliyan.com/">Iliyan Georgiev</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="https://tuanfeng.github.io/">Tuanfeng Y. Wang</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://krsingh.cs.ucdavis.edu/">Krishna Kumar Singh</a><sup>2</sup>,
            </span>
            <span class="author-block">
              <a href="https://scholar.google.com/citations?user=MHet2VoAAAAJ&hl=en">Ulrich Neumann</a><sup>1+</sup>
            </span>
            <span class="author-block">
              <a href="https://gorokee.github.io/jsyoon/">Jae Shin Yoon</a><sup>2+</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Southern California,</span>
            <span class="author-block"><sup>2</sup>Adobe Research,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup></sup>ICCV (2025)</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2508.01464"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <!-- arxiv -->
                <a href="https://arxiv.org/abs/2508.01464" 
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://youtu.be/0qRcjTw7-YU"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/Zerg-Overmind/Can3Tok"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
        <img src="static/images/teaser.png" alt="empty">
        <p>
          <br />
         We present inference results on unseen 3DGS inputs. Previous 3D VAE models even fail to converge on over a thousand scene-level 3DGS representations <strong>during training</strong>.
        </p>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="60%" weight="60">
        <source src="./static/videos/teaser_video.mp4"
                type="video/mp4">
      </video>
      
    </div>
  </div>
</section> -->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            3D generation has made significant progress, however, it
            still largely remains at the object-level. Feedforward 3D
            scene-level generation has been rarely explored due to the
            lack of models capable of scaling-up latent representation
            learning on 3D scene-level data. Unlike object-level generative models, which are trained on well-labeled 3D data
            in a bounded canonical space, scene-level generations with
            3D scenes represented by 3D Gaussian Splatting (3DGS)
            are unbounded and exhibit scale inconsistency across different scenes, making unified latent representation learning
            for generative purposes extremely challenging. In this paper, we introduce Can3Tok, the first 3D scene-level varia-
            tional autoencoder (VAE) capable of encoding a large number of Gaussian primitives into a low-dimensional latent
            embedding, which effectively captures both semantic and
            spatial information of the inputs. Beyond model design,
            we propose a general pipeline for 3D scene data processing to address scale inconsistency issue. We validate our
            method on the recent scene-level 3D dataset DL3DV-10K,
            where we found that only Can3Tok successfully generalizes to novel 3D scenes, while compared methods fail to
            converge on even a few hundred scene inputs during training and exhibit zero generalization ability during inference.
            Finally, we demonstrate image-to-3DGS and text-to-3DGS
            generation as our applications to demonstrate its ability
            to facilitate downstream generation tasks.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="./static/images/white.mp4"
                  frameborder="0" allow="autoplay; encrypted-media" ></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Inconsistent Scale Across Different Scenes</h2>
        <!-- Interpolating. -->
        <img src="./static/images/scene_variation.png" class="interpolation-image" alt="Interpolate start reference image."/>
        <p>
          <br />
         Scene-level representations exhibit varying levels of detail. Also, SfM point clouds and camera poses optimized from COLMAP are inherently not guaranteed to be in a canonical scale, making it difficult for VAE models to learn a unified and smooth latent space across different scenes.
         Therefore, 3DGS representations optimized using camera poses and initial SfM point clouds from COLMAP or other metric-scale estimations must be normalized to ensure a unified latent representation.
        </p>
      <!--/ Interpolating. -->
      </div>
    </div>
    <!--/ Method. -->
    <br />

<section class="section">
  <div class="container is-max-desktop">
    <!-- Method. -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Model Architecture</h2>
        <!-- Interpolating. -->
        <img src="./static/images/pipeline.png" class="interpolation-image" alt="Interpolate start reference image."/>
        <p>
          <br />
          Even with the normalization, naïve convolutional-based architectures struggle to generalize on unseen scene-level 3D structure (also found by <a href="https://arxiv.org/abs/2503.14445">Bolt3D</a>), highlighting the need for improved model design. Can3Tok processes a batch of per-scene 3D Gaussians, with a batch size of B, where each scene contains the same number of
          Gaussians N. The encoder encodes input Gaussians into a low-dimensional latent space followed by a VAE reparametrization. And the
          decoder reconstructs the embeddings back into 3D space, corresponding to the original input 3D Gaussians.
        </p>
      <!--/ Interpolating. -->
      </div>
    </div>
    <!--/ Method. -->
    <br />

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Latent Analysis</h2>
        <!-- Interpolating. -->
        <img src="./static/images/latent_analysis.png" class="interpolation-image" alt="Interpolate start reference image."/>
        <p>
          <br />
          To illustrate the intuition behind our model's ability to generalize to unseen scene-level 3DGS representations, we showcase the t-SNE visualizations of the latent space of 3DGS for the
          same scene with 36 linearly interpolated SO(3) rotations from 0 to
          360 degrees. (a) (b) and (c) are the latent visualization of same scenes but different rotations around X-axis, Y-axis and Z-axis, respectively. 
          (d) is the color map corresponding to varying rotation degrees.
          All three rotations exhibit patterns of closed loops,
          demonstrating that our model preserves spatial information in the
          latent representations. In (e) and (f), <span style="color: red;">red</span> dots are latent embeddings of the same scene but with 200 random SO(3) rotations and
          <span style="color: blue;">blue</span> dots are latent embeddings of different scenes.
        </p>
      <!--/ Interpolating. -->
      </div>
    </div>

    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Semantics-aware Filtering (optional)</h2>
        <!-- Interpolating. -->
        <img src="./static/images/semantic_filtering.png" class="interpolation-image" alt="Interpolate start reference image."/>
        <p>
          3DGS reconstructions from
          general scenes often contain noise artifacts like floaters due
          to the lack of visual observations (unlike objects which are
          normally captured with sufficient views). To address this issue, we apply semantic-guided filtering
          to the raw 3DGS input to subsample as-clean-as-possible
          3DGS primitives. As shown in the figure, such semantic filtering can preserve the most semantically meaningful contents while removing the less salient
          and noisy Gaussians. Please refer to the paper for more details.
        </p>
      <!--/ Interpolating. -->
      </div>
    </div>



    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">More Results</h2>
        <!-- Interpolating. -->
        <img src="./static/images/more_results.png" class="interpolation-image" alt="Interpolate start reference image."/>

      <!--/ Interpolating. -->
      </div>
    </div>

    <!--/ Method. -->
    <br />
    <!-- Results -->
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Downstreaming Applications</h2>
        <br />
        Our Can3Tok can be used for various downstream tasks. Besides 3D compression (into latents), we demonstrate two applications: text-to-3DGS generation and image-to-3DGS generation.
        <br />

        <br />
        <h3 class="title is-4">1. Text-to-3D Generation (Sec. 4.6)</h3>
        <img src="./static/images/text-to-3d.png" class="interpolation-image" alt="Empty"/> 
        <br />
        <br />
        <h3 class="title is-4">2. Image-to-3D Generation (Sec. 4.6 and Appendix)</h3>
        <img src="./static/images/image-to-3d.png" class="interpolation-image" alt="Empty"/>
        <br />
      
    

         <br /> 
        <h2 class="title is-3">Acknowledgments</h2>
        <p>
          <br />
          We thank the following great works <a href="https://arxiv.org/abs/2107.14795">Perceiver IO</a>, <a href="https://github.com/NeuralCarver/Michelangelo">Michelangelo</a>, <a href="https://github.com/luca-medeiros/lang-segment-anything">LangSAM</a>, <a href="https://github.com/POSTECH-CVLab/point-transformer">PointTransformer</a>, <a href="https://dl3dv-10k.github.io/DL3DV-10K/">DL3DV-10K dataset</a> and <a href="https://github.com/graphdeco-inria/gaussian-splatting">3DGS</a> for their codes.
        </p>
      </div>
    </div>
  </div>
</section>


<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@INPROCEEDINGS{gao2023ICCV,
  author = {Quankai Gao and Iliyan Georgiev and Tuanfeng Y. Wang and Krishna Kumar Singh and Ulrich Neumann and Jae Shin Yoon},
  title = {Can3Tok: Canonical 3D Tokenization and Latent Modeling of Scene-Level 3D Gaussians},
  booktitle = {Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)},
  year = {2025}
}</code></pre>
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
